{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE 算法评估"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RJieba 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T15:02:09.186326Z",
     "start_time": "2023-04-22T15:02:08.935382Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example: 记者1月15日从上海铁路局淮南西站获悉,淮南铁路预计春运期间发送旅客33.3万人。预计客流最高峰日为2月6日,将发送旅客1.8万人,淮南东开往\n",
      "rjieba:  记者|1|月|15|日|从|上海铁路局|淮南|西站|获悉|,|淮南|铁路|预计|春运期间|发送|旅客|33.3|万人|。|预计|客流|最高峰|日为|2|月|6|日|,|将|发送|旅客|1.8|万人|,|淮南|东|开往\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import rjieba\n",
    "\n",
    "data_path = \"./data/\"\n",
    "train_file = os.path.join(data_path, \"train_BPE.txt\")\n",
    "test_file = os.path.join(data_path, \"test_BPE.txt\")\n",
    "\n",
    "vocab_size = int(2e4)\n",
    "min_freq = 1\n",
    "cut_res = {}\n",
    "\n",
    "with open(train_file) as f:\n",
    "    train_data = f.read()\n",
    "\n",
    "with open(test_file) as f:\n",
    "    test_data = f.read()\n",
    "    test_data = test_data.replace(\" \", \"\")\n",
    "    cut_res[\"rjieba\"] = rjieba.cut(test_data)\n",
    "    example = \"\".join(cut_res[\"rjieba\"][:38])\n",
    "    print(f\"example: {example}\")\n",
    "    print(f\"rjieba:  {'|'.join(rjieba.cut(example))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 腾讯 Texsmart 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T14:28:39.478185Z",
     "start_time": "2023-04-22T14:28:39.188988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texsmart word_list: 记者|1|月|15|日|从|上海|铁路|局|淮南|西站|获悉|,|淮南|铁路|预计|春运|期间|发送|旅客|33|.|3|万|人|。|预计|客流|最|高|峰|日|为|2|月|6|日|,|将|发送|旅客|1|.|8|万|人|,|淮南|东|开往\n",
      "texsmart phrase_list: 记者|1月15日|从|上海铁路局|淮南|西站|获悉|,|淮南铁路|预计|春运期间|发送|旅客|33.3|万|人|。|预计|客流|最高峰|日|为|2月6日|,|将|发送|旅客|1.8|万|人|,|淮南东|开往\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "def texsmart_cut(text: str):\n",
    "    if text == \"\":\n",
    "        return {\n",
    "            \"word_list\": [],\n",
    "            \"phrase_list\": []\n",
    "        }\n",
    "\n",
    "    obj = {\"str\": text}\n",
    "    req_str = json.dumps(obj).encode()\n",
    "\n",
    "    url = \"https://texsmart.qq.com/api\"\n",
    "    \n",
    "    r = requests.post(url, data=req_str)\n",
    "    r.encoding = \"utf-8\"\n",
    "    res = r.json()\n",
    "    return {\n",
    "        \"word_list\": list(map(lambda x: x['str'], res['word_list'])),\n",
    "        \"phrase_list\": list(map(lambda x: x['str'], res['phrase_list']))\n",
    "    }\n",
    "t = texsmart_cut(example)\n",
    "print(f\"texsmart word_list: {'|'.join(t['word_list'])}\")\n",
    "print(f\"texsmart phrase_list: {'|'.join(t['phrase_list'])}\")\n",
    "\n",
    "if os.path.exists(\"output/testsmart_word.json\") and os.path.exists(\"output/testsmart_phrase.json\"):\n",
    "    with open(\"output/testsmart_word.json\") as f:\n",
    "        cut_res[\"T word\"] = json.load(f)\n",
    "    with open(\"output/testsmart_phrase.json\") as f:\n",
    "        cut_res[\"T phrase\"] = json.load(f)\n",
    "else:\n",
    "    word_list = []\n",
    "    phrase_list = []\n",
    "    for data in tqdm(test_data.split(\"\\n\")):\n",
    "        res = texsmart_cut(data)\n",
    "        word_list.extend(res[\"word_list\"] + ['\\n'])\n",
    "        phrase_list.extend(res[\"phrase_list\"] + ['\\n'])\n",
    "    cut_res[\"T word\"] = word_list[:-1]\n",
    "    cut_res[\"T phrase\"] = phrase_list[:-1]\n",
    "    with open(\"output/testsmart_word.json\", \"w\") as f:\n",
    "        json.dump(cut_res[\"textsmart word\"], f, ensure_ascii=False)\n",
    "    with open(\"output/testsmart_phrase.json\", \"w\") as f:\n",
    "        json.dump(cut_res[\"textsmart phrase\"], f, ensure_ascii=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T14:28:42.687265Z",
     "start_time": "2023-04-22T14:28:39.663482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "记者|1月|15日|从|上海|铁路|局|淮|南|西|站|获悉|,|淮|南|铁路|预计|春运期间|发送|旅客|33|.|3|万人|。|预计|客流|最高|峰|日|为|2月|6日|,|将|发送|旅客|1|.|8|万人|,|淮|南|东|开往\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "# set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"<UNK>\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=vocab_size,\n",
    "    show_progress=False,\n",
    "    special_tokens=[\"<UNK>\", \"\\n\"],\n",
    "    min_frequency=min_freq\n",
    ")\n",
    "tokenizer.train_from_iterator([train_data], trainer=trainer)\n",
    "\n",
    "def hg_cut(tokenizer, text):\n",
    "    tokens = tokenizer.encode(text).tokens\n",
    "    pos = 0\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] == \"<UNK>\":\n",
    "            tokens[i] = text[pos]\n",
    "            pos += 1\n",
    "        else:\n",
    "            pos += len(tokens[i])\n",
    "    return tokens\n",
    "output = hg_cut(tokenizer, example)\n",
    "print(\"|\".join(output))\n",
    "cut_res[\"HG\"] = hg_cut(tokenizer, test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subword NMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T15:03:57.413915Z",
     "start_time": "2023-04-22T15:02:53.981654Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "nmt_log_path = f\"./output/bpe{int(vocab_size // 1000)}k.txt\"\n",
    "nmt_vocab_path = f\"./output/nmt_vocab.txt\"\n",
    "if not (os.path.exists(nmt_log_path) and os.path.exists(nmt_vocab_path)):\n",
    "    subprocess.run(['subword-nmt', 'learn-joint-bpe-and-vocab', '-i', train_file, '--symbols', str(vocab_size), '-o', nmt_log_path, '--write-vocabulary', nmt_vocab_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'记者|1月|15日|从|上海|铁路|局|淮南|西|站|获悉|,|淮南|铁路|预计|春运|期间|发送|旅客|3|3|.|3|万人|。|预计|客流|最高|峰|日|为|2月|6日|,|将|发送|旅客|1|.|8万|人|,|淮南|东|开|往'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re                                                                                                                                                                                              \n",
    "puncs_zh = [' ', '。', '，', '？', '！', '；', '：', '、', '（', '）', '「',\n",
    "            '」', '“', '”', '‘', '’', '《', '》', '【', '】', '…', '—', '～']\n",
    "puncs_en = ['.', ',', '?', '!', ';', ':', \n",
    "            '(', ')', '\"', '\"', '\\'', '\\'', '<', '>', '[', ']', '.','~']\n",
    "puncs = {*puncs_zh, *puncs_en, \"\\n\", \"\\t\"}\n",
    "pattern = re.compile(f\"({'|'.join(map(re.escape, puncs))})\")\n",
    "\n",
    "def split_with_puncs(text: str) -> list:    \n",
    "    return list(filter(None, pattern.split(text)))   \n",
    "\n",
    "def nmt_cut(txt):\n",
    "    tmp_in_file =\"tmp/nmt_in.txt\"\n",
    "    tmp_out_file =\"tmp/nmt_out.txt\"\n",
    "    with open(tmp_in_file, \"w\") as f:\n",
    "        f.write(txt)\n",
    "    subprocess.run(['subword-nmt', 'apply-bpe', '-c', nmt_log_path, '--vocabulary',nmt_vocab_path, '-i', tmp_in_file, '-o', tmp_out_file, '--dropout', '0'])\n",
    "    with open(tmp_out_file) as f:\n",
    "        res = []\n",
    "        for line in f.readlines():\n",
    "            for word in line.split(\"@@ \"):\n",
    "                res.extend(split_with_puncs(word))\n",
    "    return res\n",
    "cut_res[\"subword nmt\"] = nmt_cut(test_data)\n",
    "\"|\".join(nmt_cut(example))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient BPE (My Implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T15:02:35.134809Z",
     "start_time": "2023-04-22T15:02:29.355616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using custom pairwise\n",
      "forward \t记者|1月|15日|从|上海|铁路|局|淮南|西|站|获悉|,|淮南|铁路|预计|春运期间|发送|旅客|33|.|3万人|。|预计|客流|最高|峰|日|为2|月|6日|,|将|发送|旅客|1|.|8万|人|,|淮南|东|开往\n",
      "backward\t记者|1月|15日|从|上海|铁路|局|淮南|西|站|获悉|,|淮南|铁路|预计|春运期间|发送|旅客|33|.|3万人|。|预计|客流|最|高峰|日|为|2月|6日|,|将|发送|旅客|1|.|8|万人|,|淮南|东|开往\n",
      "bidirect\t记者|1月|15日|从|上海|铁路|局|淮南|西|站|获悉|,|淮南|铁路|预计|春运期间|发送|旅客|33|.|3万人|。|预计|客流|最|高峰|日|为|2月|6日|,|将|发送|旅客|1|.|8|万人|,|淮南|东|开往\n",
      "bpe raw \t记者|1月|15日|从|上海|铁路|局|淮南|西|站|获悉|,|淮南|铁路|预计|春运期间|发送|旅客|33|.|3万人|。|预计|客流|最高|峰|日|为|2月|6日|,|将|发送|旅客|1|.|8|万人|,|淮南|东|开往\n"
     ]
    }
   ],
   "source": [
    "from ebpe import BPETrainer, BPE\n",
    "tokenizer: BPE = BPETrainer(vocab_size=vocab_size, min_freq=min_freq, single_char=False, compress_threshold=0.3).train_from_iter([train_data])\n",
    "tokenizer\n",
    "print(\"forward \\t\" + \"|\".join(tokenizer.decode_forward(example)))\n",
    "print(\"backward\\t\" + \"|\".join(tokenizer.decode_backward(example)))\n",
    "print(\"bidirect\\t\" + \"|\".join(tokenizer.decode_bidirectional(example)))\n",
    "print(\"bpe raw \\t\" + \"|\".join(tokenizer.tokenize(example)))\n",
    "cut_res[\"forward*\"] = tokenizer.decode_forward(test_data)\n",
    "cut_res[\"backward*\"] = tokenizer.decode_backward(test_data)\n",
    "cut_res[\"bidirect*\"] = tokenizer.decode_bidirectional(test_data)\n",
    "cut_res[\"bpe_raw*\"] = tokenizer.tokenize(test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate (F1 Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T14:28:39.663310Z",
     "start_time": "2023-04-22T14:28:39.483196Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rjieba</th>\n",
       "      <th>T word</th>\n",
       "      <th>T phrase</th>\n",
       "      <th>HG</th>\n",
       "      <th>subword nmt</th>\n",
       "      <th>forward*</th>\n",
       "      <th>backward*</th>\n",
       "      <th>bidirect*</th>\n",
       "      <th>bpe_raw*</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rjieba</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.809689</td>\n",
       "      <td>0.802598</td>\n",
       "      <td>0.646774</td>\n",
       "      <td>0.667960</td>\n",
       "      <td>0.665888</td>\n",
       "      <td>0.660791</td>\n",
       "      <td>0.663305</td>\n",
       "      <td>0.668211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T word</th>\n",
       "      <td>0.809689</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.755691</td>\n",
       "      <td>0.701192</td>\n",
       "      <td>0.721894</td>\n",
       "      <td>0.703979</td>\n",
       "      <td>0.698688</td>\n",
       "      <td>0.700842</td>\n",
       "      <td>0.707145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T phrase</th>\n",
       "      <td>0.802598</td>\n",
       "      <td>0.755691</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.624379</td>\n",
       "      <td>0.643529</td>\n",
       "      <td>0.646300</td>\n",
       "      <td>0.639902</td>\n",
       "      <td>0.643001</td>\n",
       "      <td>0.648838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG</th>\n",
       "      <td>0.646774</td>\n",
       "      <td>0.701192</td>\n",
       "      <td>0.624379</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923624</td>\n",
       "      <td>0.907419</td>\n",
       "      <td>0.905487</td>\n",
       "      <td>0.909452</td>\n",
       "      <td>0.944053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subword nmt</th>\n",
       "      <td>0.667960</td>\n",
       "      <td>0.721894</td>\n",
       "      <td>0.643529</td>\n",
       "      <td>0.923624</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.897476</td>\n",
       "      <td>0.896734</td>\n",
       "      <td>0.899905</td>\n",
       "      <td>0.928790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forward*</th>\n",
       "      <td>0.665888</td>\n",
       "      <td>0.703979</td>\n",
       "      <td>0.646300</td>\n",
       "      <td>0.907419</td>\n",
       "      <td>0.897476</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.930910</td>\n",
       "      <td>0.952883</td>\n",
       "      <td>0.958321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>backward*</th>\n",
       "      <td>0.660791</td>\n",
       "      <td>0.698688</td>\n",
       "      <td>0.639902</td>\n",
       "      <td>0.905487</td>\n",
       "      <td>0.896734</td>\n",
       "      <td>0.930910</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977997</td>\n",
       "      <td>0.958039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bidirect*</th>\n",
       "      <td>0.663305</td>\n",
       "      <td>0.700842</td>\n",
       "      <td>0.643001</td>\n",
       "      <td>0.909452</td>\n",
       "      <td>0.899905</td>\n",
       "      <td>0.952883</td>\n",
       "      <td>0.977997</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bpe_raw*</th>\n",
       "      <td>0.668211</td>\n",
       "      <td>0.707145</td>\n",
       "      <td>0.648838</td>\n",
       "      <td>0.944053</td>\n",
       "      <td>0.928790</td>\n",
       "      <td>0.958321</td>\n",
       "      <td>0.958039</td>\n",
       "      <td>0.962347</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               rjieba    T word  T phrase        HG  subword nmt  forward*  \\\n",
       "rjieba       1.000000  0.809689  0.802598  0.646774     0.667960  0.665888   \n",
       "T word       0.809689  1.000000  0.755691  0.701192     0.721894  0.703979   \n",
       "T phrase     0.802598  0.755691  1.000000  0.624379     0.643529  0.646300   \n",
       "HG           0.646774  0.701192  0.624379  1.000000     0.923624  0.907419   \n",
       "subword nmt  0.667960  0.721894  0.643529  0.923624     1.000000  0.897476   \n",
       "forward*     0.665888  0.703979  0.646300  0.907419     0.897476  1.000000   \n",
       "backward*    0.660791  0.698688  0.639902  0.905487     0.896734  0.930910   \n",
       "bidirect*    0.663305  0.700842  0.643001  0.909452     0.899905  0.952883   \n",
       "bpe_raw*     0.668211  0.707145  0.648838  0.944053     0.928790  0.958321   \n",
       "\n",
       "             backward*  bidirect*  bpe_raw*  \n",
       "rjieba        0.660791   0.663305  0.668211  \n",
       "T word        0.698688   0.700842  0.707145  \n",
       "T phrase      0.639902   0.643001  0.648838  \n",
       "HG            0.905487   0.909452  0.944053  \n",
       "subword nmt   0.896734   0.899905  0.928790  \n",
       "forward*      0.930910   0.952883  0.958321  \n",
       "backward*     1.000000   0.977997  0.958039  \n",
       "bidirect*     0.977997   1.000000  0.962347  \n",
       "bpe_raw*      0.958039   0.962347  1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Set, Tuple, List, Dict\n",
    "import pandas as  pd\n",
    "def words2pos(words: List[str]):\n",
    "    res = []\n",
    "    pos = 0\n",
    "    for word in words:\n",
    "        nxt_pos = pos + len(word)\n",
    "        res.append((pos, nxt_pos))\n",
    "        pos = nxt_pos\n",
    "    return set(res)\n",
    "\n",
    "def F1_score(pred: Set[Tuple[int, int]], true: Set[Tuple[int, int]]):\n",
    "    \"\"\"\n",
    "    pred: 预测的分词结果\n",
    "    true: 真实的分词结果\n",
    "    \"\"\"\n",
    "    TP = len(pred & true)\n",
    "    precision = TP / len(pred)\n",
    "    recall = TP / len(true)\n",
    "    F1 = 2 * precision * recall / (precision + recall)\n",
    "    return F1\n",
    "\n",
    "def evaluate(data: Dict[str, List[str]]):\n",
    "    data = {\n",
    "        k: words2pos(v)\n",
    "        for k, v  in data.items()\n",
    "    }\n",
    "    return pd.DataFrame({\n",
    "        pred_name: {\n",
    "            true_name: F1_score(pred, true) \n",
    "            for true_name, true in data.items()\n",
    "        } for pred_name, pred in data.items()\n",
    "    })\n",
    "\n",
    "evaluate(cut_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
